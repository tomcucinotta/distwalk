.TH DW_NODE 1 "August 2025" "dw_node manual" "User Manuals"

.SH NAME
dw_node - high performance server with multi-protocol and multi-core support

.SH SYNOPSIS
.B dw_node
[\fIOPTIONS\fR]...

.SH DESCRIPTION
dw_node is a flexible and high-performance server that supports multiple communication protocols (TCP, UDP, SSL), advanced scheduling, CPU affinity,
and optimized disk/storage access for latency-sensitive workloads.

.SH OPTIONS

.TP
.BR -b,\ -\-bind-addr "=[ tcp | udp | ssl:[//] ] [ host ] [ :port ]"
Set the bind name or IP address, port, and communication protocol. All three parts of the argument are optional:
.nf
  - if the protocol is not specified, then TCP is assumed by default
  - if the host is not specified, then localhost is assumed by default
  - if the port is not specified, then 7891 is assumed by default
.ni

This option can be used multiple times, to let the same `dw_node` instance bind to multiple IPs, ports or protocols.

Not using this option, is equivalent to calling dw_node with `-b tcp://localhost:7891`.

.TP
.BR --backlog-length = \fIn\fR ", " --bl = \fIn\fR
Set the maximum number of pending incoming connections, for each bound socket, i.e., the argument used when calling listen(). See also the accept mode option described above.

.TP
.BR --no-delay "= \fI0|1\fR, " --nd "=\fI0|1\fR"
Enable or disable the TCP_NODELAY socket option (enabled by default).

.TP
.BR --nt = \fIn\fR ", " --num-threads = \fIn\fR
Set the number of worker threads (defaults to one worker thread).

.TP
.BR -c,\ \--thread-affinity "=auto|cX,cZ[,cA-cD[:step]]"
Enable thread-to-core pinning through affinity masks. The mask is specified with the usual core-list syntax, as a comma-separated list of core ranges, with the optional use of a colon followed by a core step in a specified range, if desired. If the number of worker threads (see --nt) is higher than the cores in the affinity list, then the list is reused circularly from the beginning (this allows for pinning 2, 3 or as many threads as desired to a single physical core). The special value "auto" sets and affinity of 0-n, where n is equal to the number of specified threads minus one. By default, no affinity is set for worker threads.

.TP
.BR --sched-policy "=other[:nice]|rr:rtprio|fifo:rtprio|dl:runtime_us,dline_us"
Set the scheduling policy (defaults to other), and its parameters:
.nf
   `other`: use the default Linux SCHED_OTHER scheduler, where the optional nice parameter can be used to customize the niceness level
   `rr`, `fifo`: use the SCHED_RR or SCHED_FIFO real-time scheduling classes, with the specified real-time priority number, in the range 1-99
   `dl`: use the SCHED_DEADLINE EDF/CBS-based real-time scheduler available in the mainline Linux kernel, using the specified reservation runtime and deadline, which is set equal to the reservation period.
.ni

Run the node in privileged mode if you wish to use `rr`, `fifo`, `dl` or `other` with negative niceness. Alternatively, configure unprivileged access to priorities and nice values in `/etc/security/limits.conf`. If you are planning to use `dl` together with `--thread-affinity`, you need to disable the admission control with:
.nf
    echo -1 > /proc/sys/kernel/sched_rt_runtime_us
.ni
(Default value is 950000)

.TP
.BR --wait-spin,\ \--ws
Tell each worker thread to perform a busy-wait loop, till the next incoming request on any of the monitored sockets, instead of sleeping. This allows the worker threads to be always ready, so to save wake-up from idle, and context switch overheads, thus it is useful in scenarios seeking extremely low latency. However, it causes each worker thread to take 100% of a CPU, regardless on the amount of traffic hitting it.

.TP
.BR --wait-spin-storage,\ \--wss
Tell the storage thread to perform a busy-wait loop instead of blocking while waiting for new LOAD/STORE requests. Similarly to --wait-spin, this will reduce latency but will cause the storage thread to take 100% of a CPU.

.TP
.BR --loops-per-usec = \fIvalue\fR
Switch the implementation of COMPUTE commands from computing for exactly the amount of time the -C client option requires, regardless of the underlying CPU frequency, to the specified fixed number of repeatitions of a simple compute-intensive loop, which results in computations whose duration varies with the CPU frequency. Specifying the special value 0, lets the program benchmark itself for a fraction of a second when launched, causing the CPU to scale-up its frequency, then store number of performed loops per second in the $HOME/.dw_loops_per_usec file, then this value is reused for any successive invocation of dw_node with 0 as value.

.TP
.BR -a,\ \--accept-mode "=child|shared|parent"
Set the server accept mode:
.nf
   `child`: each worker thread calls accept() independently from its own bound socket, availing of the REUSEPORT socket option
   `shared`: all worker threads accept() from the same socket, that is created and bound just once, then used by all of them
   `parent`: only the parent accept()s connection on a single bound socket, then connections are handed over to worker threads internally
.ni

.TP
.BR -p,\ \--poll-mode "=epoll | poll | select"
Set the poll mode (defaults to epoll) used by worker threads for monitoring their sockets:
.nf
   `select`: use the select() system call
   `poll`: use the poll() system call
   `epoll`: use the epoll() system call (default)
.ni

.TP
.BR -s,\ \--storage = \fIpath/to/storage/file\fR
Set the path to the file to be used for LOAD/STORE operations. On STORE, the file is automatically expanded in size, up to the maximum size specified with -m. Note that the file location allows for determining also the type of storage one wants to use, depending on whether the specified pathname is on a HDD or SSD drive, for example.

.TP
.BR -m,\ \--max-storage-size = \fInbytes\fR
Set the maximum size for the storage file used for LOAD/STORE operations. When the file offset is automatically increased and it reaches this value,
it undergoes a rewind to 0. See also the -s option.

.TP
.B --odirect
Enable direct disk access for LOAD and STORE operations (bypass read/write OS caches).

.TP
.BR --sync = \fImsec\fR
Periodically sync the written data on disk, issueing a fsync() system call on the
storage file.

.SH FILES
.TP
.I $HOME/.dw_loops_per_usec
Stores computed loop repetition rate when using
.B --loops-per-usec=0

.SH SEE ALSO
.BR dw_client (1)

.SH BUGS
For bug reports, use the issue tracker at https://github.com/tomcucinotta/distwalk/issues.

.SH COPYRIGHT
Copyright Â© 2025. All rights reserved.
